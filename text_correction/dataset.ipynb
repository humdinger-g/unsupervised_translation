{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "!python -m spacy download ru_core_news_sm en_core_web_sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import pymorphy3\n",
    "from datasets import (Dataset, \n",
    "                      DatasetDict, \n",
    "                      load_dataset, \n",
    "                      load_from_disk, \n",
    "                      concatenate_datasets)\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "data_path = os.path.abspath(os.path.join('..', 'fasttext_data'))\n",
    "if data_path not in sys.path:\n",
    "    sys.path.append(data_path)\n",
    "\n",
    "path = '../fasttext_data'\n",
    "embeds = torch.load(os.path.join(path, 'ru_embeds.pt'))[:10000]\n",
    "words = np.load(os.path.join(path, 'ru_words.npy'))[:10000]\n",
    "w2idx = {w:i for i, w in enumerate(words)}\n",
    "\n",
    "def get_synonym_ru(word, embeds=embeds, words=words, w2idx=w2idx):\n",
    "    word = word.lower()\n",
    "    if not word in w2idx:\n",
    "        return '<NULL>'\n",
    "    embed = embeds[w2idx[word]]\n",
    "    synonyms_idx = torch.topk(torch.cosine_similarity(embeds, embed), 10)[1]\n",
    "    idx = np.random.choice(synonyms_idx[1:])\n",
    "    return words[idx]\n",
    "\n",
    "synonyms_ru = {}\n",
    "for word in tqdm(words):\n",
    "    syn = get_synonym_ru(word)\n",
    "    synonyms_ru[word] = syn\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for an English word using WordNet.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')\n",
    "            if synonym != word:\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text in the original dataset will be processed with `degrade_text` function which will\n",
    "1. delete punctuation\n",
    "2. lemmatize words\n",
    "3. lower words\n",
    "4. randomly shuffle words in the sliding window of length 5\n",
    "5. change random words with their synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_text(text, lang='ru'):\n",
    "    text.replace(\"\\n\", \"\")\n",
    "    nlp = nlp_ru if lang == 'ru' else nlp_en\n",
    "    doc = nlp(text)\n",
    "    degraded_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        elif token.pos_ in {\"DET\", \"ADP\"}:\n",
    "            continue\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            degraded_tokens.append(token.lemma_)\n",
    "        elif token.pos_ == \"NOUN\" and token.tag_ == \"NNS\":\n",
    "            degraded_tokens.append(token.lemma_)\n",
    "        elif token.pos_ == \"PRON\":\n",
    "            continue\n",
    "        else:\n",
    "            degraded_tokens.append(token.text.lower())\n",
    "    \n",
    "    chunk_size = 3\n",
    "    chunks = [degraded_tokens[i:i + chunk_size] for i in range(0, len(degraded_tokens), chunk_size)]\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) > 1 and random.random() < 0.3:\n",
    "            random.shuffle(chunk)\n",
    "    \n",
    "    degraded_tokens = [item for chunk in chunks for item in chunk]\n",
    "\n",
    "    for i in range(len(degraded_tokens)):\n",
    "        if random.random() < 0.2 and lang == 'en':\n",
    "            synonyms = get_synonyms(degraded_tokens[i])\n",
    "            if synonyms:\n",
    "                degraded_tokens[i] = random.choice(synonyms)\n",
    "        elif random.random() < 0.2 and lang == 'ru' and degraded_tokens[i] in synonyms_ru:\n",
    "            degraded_tokens[i] = synonyms_ru[degraded_tokens[i].lower()]\n",
    "            \n",
    "        if random.random() < 0.02:\n",
    "            degraded_tokens[i] = '<NULL>'\n",
    "    \n",
    "    degraded_text = \" \".join(degraded_tokens)\n",
    "    return degraded_text\n",
    "\n",
    "original_text = 'Меня зовут по имени каждый раз, когда ко мне обращаются'\n",
    "degraded_text = degrade_text(original_text, lang='ru')\n",
    "print(\"Original Text: \", original_text)\n",
    "print(\"Degraded Text: \", degraded_text)\n",
    "\n",
    "original_text = 'I am going to school right now'\n",
    "degraded_text = degrade_text(original_text, lang='en')\n",
    "print(\"Original Text: \", original_text)\n",
    "print(\"Degraded Text: \", degraded_text)\n",
    "\n",
    "# Original Text:  Меня зовут по имени каждый раз, когда ко мне обращаются\n",
    "# Degraded Text:  звать имени разом когда обращаться\n",
    "# Original Text:  I am going to school right now\n",
    "# Degraded Text:  am crack school right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this function is applied to the merged dataset of English and Russian wikipedia texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en = load_dataset('wikipedia', language='en', date='20220301')\n",
    "dataset_ru = load_dataset('wikipedia', language=\"ru\", date=\"20240520\")\n",
    "\n",
    "def length(example, subset_ratio):\n",
    "    text = example['text']\n",
    "    words = text.split()\n",
    "    return (len(words) > 5) and (len(words) < 300) and (random.random() < subset_ratio)\n",
    "\n",
    "filtered_dataset_en = dataset_en.filter(lambda x: length(x, subset_ratio=0.1))\n",
    "filtered_dataset_ru = dataset_ru.filter(lambda x: length(x, subset_ratio=0.3))\n",
    "\n",
    "\n",
    "def preprocess_dataset(ex, lang):\n",
    "    ex['bad_text'] = [degrade_text(text, lang=lang) for text in ex['text']]\n",
    "    return ex\n",
    "\n",
    "\n",
    "final_en = filtered_dataset_en.map(lambda x: preprocess_dataset(x, lang='en'), batched=True)\n",
    "final_ru = filtered_dataset_ru.map(lambda x: preprocess_dataset(x, lang='ru'), batched=True)\n",
    "\n",
    "dataset_ru_new = final_ru['train'].add_column('lang', ['ru'] * len(final_ru['train']))\n",
    "dataset_en_new = final_en['train'].add_column('lang', ['en'] * len(final_en['train']))\n",
    "\n",
    "concatenated_dataset = concatenate_datasets([dataset_ru_new, dataset_en_new])\n",
    "\n",
    "final_dataset = concatenated_dataset.shuffle(seed=42)\n",
    "#final_dataset.push_to_hub(\"gudleifrr/text-correction-en-ru\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsuper_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
